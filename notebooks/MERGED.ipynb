{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "962ca8d5",
   "metadata": {},
   "source": [
    "\n",
    "### Composer Classification â€” LSTM + CNN/Hybrid (Balanced Test, Merged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82618ed0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e4ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available()\n",
    "                      else \"cuda\" if torch.cuda.is_available()\n",
    "                      else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 215 raw Beethoven files in ../data/raw_Beethoven\n",
      "Beethoven placed -> train:147  dev:64  test:4\n",
      "test/bach: 4\n",
      "test/beethoven: 4\n",
      "test/chopin: 4\n",
      "test/mozart: 4\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "# Force Beethoven test set to EXACTLY 4 files (others already have 4).\n",
    "BASE_DATA = Path(\"../data\")\n",
    "RAW_BEET  = BASE_DATA / \"raw_Beethoven\"\n",
    "LABEL     = \"beethoven\"\n",
    "\n",
    "all_beet = sorted(RAW_BEET.rglob(\"*.mid\"))\n",
    "print(f\"Found {len(all_beet)} raw Beethoven files in {RAW_BEET}\")\n",
    "\n",
    "# Pick exactly 4 for TEST (or fewer if insufficient files)\n",
    "random.seed(42)\n",
    "k_test = min(4, len(all_beet))\n",
    "test_files = random.sample(all_beet, k=k_test)\n",
    "remaining  = [p for p in all_beet if p not in test_files]\n",
    "\n",
    "# Split remaining -> train/dev (70/30)\n",
    "train_files, dev_files = train_test_split(remaining, test_size=0.30, random_state=42)\n",
    "\n",
    "def reset_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    # Remove only MIDI files (donâ€™t nuke other assets)\n",
    "    for ext in (\"*.mid\", \"*.midi\"):\n",
    "        for old in p.glob(ext):\n",
    "            old.unlink()\n",
    "\n",
    "t_train = BASE_DATA / \"train\" / LABEL\n",
    "t_dev   = BASE_DATA / \"dev\"   / LABEL\n",
    "t_test  = BASE_DATA / \"test\"  / LABEL\n",
    "\n",
    "for d in (t_train, t_dev, t_test):\n",
    "    reset_dir(d)\n",
    "\n",
    "import shutil\n",
    "for f in train_files: shutil.copy2(f, t_train / f.name)\n",
    "for f in dev_files:   shutil.copy2(f,   t_dev / f.name)\n",
    "for f in test_files:  shutil.copy2(f,  t_test / f.name)\n",
    "\n",
    "print(f\"Beethoven placed -> train:{len(train_files)}  dev:{len(dev_files)}  test:{len(test_files)}\")\n",
    "\n",
    "for c in [\"bach\",\"beethoven\",\"chopin\",\"mozart\"]:\n",
    "    n = sum(1 for _ in (BASE_DATA/\"test\"/c).rglob(\"*.mid\")) \\\n",
    "      + sum(1 for _ in (BASE_DATA/\"test\"/c).rglob(\"*.midi\"))\n",
    "    print(f\"test/{c}: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a158a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 4 old processed files. Youâ€™ll rebuild them next.\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "PROC = Path(\"../data/processed_data\")\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "to_delete = [\n",
    "    PROC/\"lstm_train.pkl\", PROC/\"lstm_dev.pkl\", PROC/\"lstm_test.pkl\",\n",
    "    PROC/\"label_encoder.pkl\"\n",
    "]\n",
    "deleted = 0\n",
    "for p in to_delete:\n",
    "    if p.exists():\n",
    "        p.unlink()\n",
    "        deleted += 1\n",
    "print(f\"Deleted {deleted} old processed files. Youâ€™ll rebuild them next.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f85f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "# Install pretty_midi\n",
    "try:\n",
    "    import pretty_midi\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pretty_midi\"])\n",
    "    import pretty_midi\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978fa641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built windows: Train (1529502, 50, 3) Dev (576863, 50, 3) Test (56063, 50, 3)\n",
      "Train {'bach': 85611, 'beethoven': 1049405, 'chopin': 104020, 'mozart': 290466}\n",
      "Dev {'bach': 11015, 'beethoven': 535829, 'chopin': 9309, 'mozart': 20710}\n",
      "Test {'bach': 11313, 'beethoven': 16768, 'chopin': 9581, 'mozart': 18401}\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "# Build LSTM windows (50x3: pitch, step, duration)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "BASE = Path(\"../data\")\n",
    "PROC = BASE / \"processed_data\"\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LABELS = [\"bach\",\"beethoven\",\"chopin\",\"mozart\"]  \n",
    "le = LabelEncoder()\n",
    "le.fit(LABELS)\n",
    "\n",
    "WINDOW_LEN = 50\n",
    "STRIDE     = 1  \n",
    "\n",
    "def midi_to_events(fp):\n",
    "    \"\"\"\n",
    "    Returns an array of shape (N, 3): [pitch, step, duration] in seconds.\n",
    "    step = time since previous note onset; duration = note end - start.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pm = pretty_midi.PrettyMIDI(str(fp))\n",
    "        notes = []\n",
    "        for inst in pm.instruments:\n",
    "            for n in inst.notes:\n",
    "                notes.append((n.start, n.end, n.pitch))\n",
    "        if not notes:\n",
    "            return None\n",
    "        notes.sort(key=lambda x: x[0])  \n",
    "        events = []\n",
    "        prev_onset = notes[0][0]\n",
    "        for start, end, pitch in notes:\n",
    "            step = float(max(0.0, start - prev_onset))\n",
    "            dur  = float(max(0.0, end - start))\n",
    "            events.append([float(pitch), step, dur])\n",
    "            prev_onset = start\n",
    "        return np.array(events, dtype=np.float32)\n",
    "    except Exception:\n",
    "        return None  # skip unreadable files\n",
    "\n",
    "def windows_from_events(ev, win=WINDOW_LEN, stride=STRIDE):\n",
    "    \"\"\"\n",
    "    Slice (N,3) events into (num_windows, win, 3) using a sliding window.\n",
    "    Skip files with fewer than 'win' notes.\n",
    "    \"\"\"\n",
    "    if ev is None or len(ev) < win:\n",
    "        return np.empty((0, win, 3), dtype=np.float32)\n",
    "    n = 1 + (len(ev) - win) // stride\n",
    "    out = np.empty((n, win, 3), dtype=np.float32)\n",
    "    j = 0\n",
    "    for i in range(0, len(ev) - win + 1, stride):\n",
    "        out[j] = ev[i:i+win]\n",
    "        j += 1\n",
    "    return out\n",
    "\n",
    "def build_split(split):\n",
    "    X_blocks = []\n",
    "    y_blocks = []\n",
    "    for label in LABELS:\n",
    "        folder = BASE / split / label\n",
    "        files = list(folder.rglob(\"*.mid\")) + list(folder.rglob(\"*.midi\"))\n",
    "        for fp in files:\n",
    "            ev = midi_to_events(fp)\n",
    "            Xw = windows_from_events(ev, WINDOW_LEN, STRIDE)\n",
    "            if Xw.shape[0] > 0:\n",
    "                X_blocks.append(Xw)\n",
    "                y_blocks.append(\n",
    "                    np.full((Xw.shape[0],), le.transform([label])[0], dtype=np.int64)\n",
    "                )\n",
    "    if X_blocks:\n",
    "        X = np.concatenate(X_blocks, axis=0)\n",
    "        y = np.concatenate(y_blocks, axis=0)\n",
    "    else:\n",
    "        X = np.empty((0, WINDOW_LEN, 3), dtype=np.float32)\n",
    "        y = np.empty((0,), dtype=np.int64)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = build_split(\"train\")\n",
    "X_dev,   y_dev   = build_split(\"dev\")\n",
    "X_test,  y_test  = build_split(\"test\")\n",
    "\n",
    "print(\"Built windows:\",\n",
    "      \"Train\", X_train.shape, \"Dev\", X_dev.shape, \"Test\", X_test.shape)\n",
    "\n",
    "with open(PROC/\"lstm_train.pkl\",\"wb\") as f: pickle.dump((X_train, y_train), f)\n",
    "with open(PROC/\"lstm_dev.pkl\",\"wb\")   as f: pickle.dump((X_dev,   y_dev),   f)\n",
    "with open(PROC/\"lstm_test.pkl\",\"wb\")  as f: pickle.dump((X_test,  y_test),  f)\n",
    "with open(PROC/\"label_encoder.pkl\",\"wb\") as f: pickle.dump(le, f)\n",
    "\n",
    "for name, y in [(\"Train\", y_train), (\"Dev\", y_dev), (\"Test\", y_test)]:\n",
    "    cnt = Counter(y.tolist())\n",
    "    print(name, {LABELS[i]: cnt.get(i, 0) for i in range(len(LABELS))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe6faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes â†’ Train/Dev/Test: (1529502, 50, 3) (576863, 50, 3) (56063, 50, 3)\n",
      "Classes: ['bach' 'beethoven' 'chopin' 'mozart']\n",
      "Train {'bach': 85611, 'beethoven': 1049405, 'chopin': 104020, 'mozart': 290466}\n",
      "Dev {'bach': 11015, 'beethoven': 535829, 'chopin': 9309, 'mozart': 20710}\n",
      "Test (before balance) {'bach': 11313, 'beethoven': 16768, 'chopin': 9581, 'mozart': 18401}\n",
      "Test (balanced in memory) {'bach': 9581, 'beethoven': 9581, 'chopin': 9581, 'mozart': 9581}\n"
     ]
    }
   ],
   "source": [
    "# 7\n",
    "PROC = Path(\"../data/processed_data\")\n",
    "with open(PROC/\"lstm_train.pkl\",\"rb\") as f:\n",
    "    X_train, y_train = pickle.load(f)\n",
    "with open(PROC/\"lstm_dev.pkl\",\"rb\") as f:\n",
    "    X_dev,   y_dev   = pickle.load(f)\n",
    "with open(PROC/\"lstm_test.pkl\",\"rb\") as f:\n",
    "    X_test,  y_test  = pickle.load(f)\n",
    "with open(PROC/\"label_encoder.pkl\",\"rb\") as f:\n",
    "    le = pickle.load(f)\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "input_size  = X_train.shape[2]\n",
    "\n",
    "print(\"Shapes â†’ Train/Dev/Test:\", X_train.shape, X_dev.shape, X_test.shape)\n",
    "print(\"Classes:\", le.classes_)\n",
    "\n",
    "# Show counts\n",
    "def show_counts(name, y):\n",
    "    cnt = Counter(y.tolist())\n",
    "    print(name, {le.classes_[i]: cnt.get(i,0) for i in range(num_classes)})\n",
    "\n",
    "show_counts(\"Train\", y_train)\n",
    "show_counts(\"Dev\",   y_dev)\n",
    "show_counts(\"Test (before balance)\", y_test)\n",
    "\n",
    "# HARD STOP if any class missing in TEST\n",
    "test_cnt = Counter(y_test.tolist())\n",
    "missing  = [le.classes_[i] for i in range(num_classes) if test_cnt.get(i,0) == 0]\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"Processed TEST set is missing classes: \"\n",
    "        f\"{missing}. Re-run your preprocessing AFTER the Beethoven split.\"\n",
    "    )\n",
    "\n",
    "# In-memory balance: downsample each class to the minimum count\n",
    "rng = np.random.default_rng(42)\n",
    "target_n = min(test_cnt[i] for i in range(num_classes))\n",
    "\n",
    "keep_idx = []\n",
    "for i in range(num_classes):\n",
    "    idx_i = np.where(y_test == i)[0]\n",
    "    pick  = rng.choice(idx_i, size=target_n, replace=False)\n",
    "    keep_idx.extend(pick)\n",
    "keep_idx = np.array(keep_idx)\n",
    "\n",
    "X_test_bal = X_test[keep_idx]\n",
    "y_test_bal = y_test[keep_idx]\n",
    "show_counts(\"Test (balanced in memory)\", y_test_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97fddfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM batch X: torch.Size([32, 50, 3])  y: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# 8\n",
    "# Convert to tensors\n",
    "X_tr  = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "y_tr  = torch.tensor(y_train, dtype=torch.long,    device=device)\n",
    "X_val = torch.tensor(X_dev,   dtype=torch.float32, device=device)\n",
    "y_val = torch.tensor(y_dev,   dtype=torch.long,    device=device)\n",
    "\n",
    "# balanced test tensors\n",
    "X_te  = torch.tensor(X_test_bal, dtype=torch.float32, device=device)\n",
    "y_te  = torch.tensor(y_test_bal, dtype=torch.long,    device=device)\n",
    "\n",
    "# Loaders\n",
    "lstm_batch_size = 32\n",
    "lstm_train_loader = DataLoader(TensorDataset(X_tr, y_tr), batch_size=lstm_batch_size, shuffle=True)\n",
    "lstm_dev_loader   = DataLoader(TensorDataset(X_val, y_val), batch_size=lstm_batch_size, shuffle=False)\n",
    "lstm_test_loader  = DataLoader(TensorDataset(X_te, y_te),   batch_size=lstm_batch_size, shuffle=False)\n",
    "\n",
    "xb, yb = next(iter(lstm_train_loader))\n",
    "print(\"LSTM batch X:\", xb.shape, \" y:\", yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "135fa4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicLSTM(\n",
      "  (lstm): LSTM(3, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (fc): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 9\n",
    "# LSTM model\n",
    "class MusicLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.fc(last)\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers  = 2\n",
    "lstm_model  = MusicLSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5b78448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM for 20 epochs (early stopping enabled)\n"
     ]
    }
   ],
   "source": [
    "# 10\n",
    "# LSTM training setup\n",
    "import time\n",
    "lstm_lr     = 1e-3\n",
    "lstm_epochs = 20  # you can lower while iterating\n",
    "\n",
    "# Early stopping (on dev accuracy)\n",
    "es_patience  = 3          # stop after 3 epochs without improvement\n",
    "es_min_delta = 1e-4       # require at least this improvement\n",
    "es_wait      = 0\n",
    "\n",
    "lstm_criterion = nn.CrossEntropyLoss()\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=lstm_lr)\n",
    "lstm_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    lstm_optimizer, mode=\"max\", factor=0.5, patience=2\n",
    ")\n",
    "\n",
    "best_dev_acc = -float(\"inf\")\n",
    "print(\"Training LSTM for\", lstm_epochs, \"epochs (early stopping enabled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f783cb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/20  Train Loss: 0.1649  Dev Loss: 0.3852  Dev Acc: 0.8604  (best: 0.0000)\n",
      "  ðŸ”– New best LSTM saved\n",
      "Epoch  2/20  Train Loss: 0.1805  Dev Loss: 0.3872  Dev Acc: 0.8547  (best: 0.8604)\n",
      "Epoch  3/20  Train Loss: 0.2009  Dev Loss: 0.3722  Dev Acc: 0.8589  (best: 0.8604)\n",
      "Epoch  4/20  Train Loss: 0.2217  Dev Loss: 0.3505  Dev Acc: 0.8676  (best: 0.8604)\n",
      "  ðŸ”– New best LSTM saved\n",
      "Epoch  5/20  Train Loss: 0.2476  Dev Loss: 0.3944  Dev Acc: 0.8393  (best: 0.8676)\n",
      "Epoch  6/20  Train Loss: 0.2952  Dev Loss: 0.3485  Dev Acc: 0.8616  (best: 0.8676)\n",
      "Epoch  7/20  Train Loss: 0.4435  Dev Loss: 0.3478  Dev Acc: 0.8601  (best: 0.8676)\n",
      "Early stopping after epoch 7 (no improvement for 3 epochs).\n"
     ]
    }
   ],
   "source": [
    "# 11\n",
    "#LSTM Training Loop\n",
    "for epoch in range(1, lstm_epochs + 1):\n",
    "    lstm_model.train()\n",
    "    train_losses = []\n",
    "    for Xb, yb in lstm_train_loader:\n",
    "        lstm_optimizer.zero_grad()\n",
    "        logits = lstm_model(Xb)\n",
    "        loss   = lstm_criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    avg_train_loss = float(np.mean(train_losses)) if train_losses else 0.0\n",
    "\n",
    "    # Dev\n",
    "    lstm_model.eval()\n",
    "    dev_preds, dev_true, dev_losses = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in lstm_dev_loader:\n",
    "            logits = lstm_model(Xb)\n",
    "            loss   = lstm_criterion(logits, yb)\n",
    "            dev_losses.append(loss.item())\n",
    "            dev_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "            dev_true.extend(yb.cpu().numpy())\n",
    "    avg_dev_loss = float(np.mean(dev_losses)) if dev_losses else 0.0\n",
    "    dev_acc      = accuracy_score(dev_true, dev_preds) if dev_true else 0.0\n",
    "    lstm_scheduler.step(dev_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}/{lstm_epochs}  Train Loss: {avg_train_loss:.4f}  \"\n",
    "          f\"Dev Loss: {avg_dev_loss:.4f}  Dev Acc: {dev_acc:.4f}  \"\n",
    "          f\"(best: {best_dev_acc if best_dev_acc!=-float('inf') else 0.0:.4f})\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if dev_acc > best_dev_acc + es_min_delta:\n",
    "        best_dev_acc = dev_acc\n",
    "        es_wait = 0\n",
    "        torch.save(lstm_model.state_dict(), PROC / \"best_lstm.pth\")\n",
    "        print(\"  ðŸ”– New best LSTM saved\")\n",
    "    else:\n",
    "        es_wait += 1\n",
    "        if es_wait >= es_patience:\n",
    "            print(f\"Early stopping after epoch {epoch} \"\n",
    "                  f\"(no improvement for {es_patience} epochs).\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96cf62cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Test Accuracy: 0.7339\n",
      "\n",
      "LSTM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        bach       0.93      0.88      0.91      9581\n",
      "   beethoven       0.54      0.93      0.68      9581\n",
      "      chopin       0.94      0.61      0.74      9581\n",
      "      mozart       0.77      0.52      0.62      9581\n",
      "\n",
      "    accuracy                           0.73     38324\n",
      "   macro avg       0.79      0.73      0.74     38324\n",
      "weighted avg       0.79      0.73      0.74     38324\n",
      "\n",
      "\n",
      "LSTM Confusion Matrix:\n",
      " [[8469  464  277  371]\n",
      " [   0 8866   13  702]\n",
      " [ 610 2745 5857  369]\n",
      " [  45 4489  112 4935]]\n"
     ]
    }
   ],
   "source": [
    "# 12\n",
    "# Load the best LSTM model for evaluation\n",
    "best_lstm = MusicLSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "best_lstm.load_state_dict(torch.load(PROC/\"best_lstm.pth\", map_location=device))\n",
    "best_lstm.eval()\n",
    "\n",
    "test_preds, test_true = [], []\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in lstm_test_loader:\n",
    "        logits = best_lstm(Xb)\n",
    "        test_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "        test_true.extend(yb.cpu().numpy())\n",
    "\n",
    "print(f\"LSTM Test Accuracy: {accuracy_score(test_true, test_preds):.4f}\\n\")\n",
    "print(\"LSTM Classification Report:\\n\",\n",
    "      classification_report(test_true, test_preds, target_names=list(le.classes_), zero_division=0))\n",
    "print(\"\\nLSTM Confusion Matrix:\\n\", confusion_matrix(test_true, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "298d0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13\n",
    "# Convert sequence to pianoroll format\n",
    "WINDOW_LEN  = X_train.shape[1]  # should be 50\n",
    "PITCH_START = 21\n",
    "PITCH_END   = 108\n",
    "NUM_PITCHES = PITCH_END - PITCH_START + 1  # 88\n",
    "\n",
    "def seq_to_pianoroll_fast(seq):\n",
    "    \"\"\"\n",
    "    seq: (T, 3) where seq[:,0] = pitch\n",
    "    returns: (1, 88, T) float32\n",
    "    \"\"\"\n",
    "    T = seq.shape[0]\n",
    "    pr = np.zeros((1, NUM_PITCHES, T), dtype=np.float32)\n",
    "    pitches = seq[:, 0].astype(np.int16)\n",
    "    idx = pitches - PITCH_START\n",
    "    t_idx = np.arange(T, dtype=np.int32)\n",
    "    mask = (idx >= 0) & (idx < NUM_PITCHES)\n",
    "    pr[0, idx[mask], t_idx[mask]] = 1.0\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f10888c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity OK; piano-rolls will be built on-the-fly per batch.\n"
     ]
    }
   ],
   "source": [
    "# 14\n",
    "_ = seq_to_pianoroll_fast(X_train[0])\n",
    "print(\"Sanity OK; piano-rolls will be built on-the-fly per batch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d3cc297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid LSTM batch: torch.Size([32, 50, 3])  CNN batch: torch.Size([32, 1, 88, 50])  y: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# 15\n",
    "# Hybrid dataset and Data Loaders for LSTM + CNN\n",
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, lstm_windows: np.ndarray, labels: np.ndarray):\n",
    "        self.lstm_windows = lstm_windows  # numpy (N, 50, 3)\n",
    "        self.labels       = labels        # numpy (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.lstm_windows[idx]                   # (50, 3) numpy\n",
    "        pr  = seq_to_pianoroll_fast(seq)               # (1, 88, 50) numpy\n",
    "        x_l = torch.from_numpy(seq).to(torch.float32)  # (50, 3)\n",
    "        x_c = torch.from_numpy(pr).to(torch.float32)   # (1, 88, 50)\n",
    "        y   = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x_l, x_c, y\n",
    "\n",
    "hyb_train_ds = HybridDataset(X_train,    y_train)\n",
    "hyb_dev_ds   = HybridDataset(X_dev,      y_dev)\n",
    "hyb_test_ds  = HybridDataset(X_test_bal, y_test_bal)\n",
    "\n",
    "# num_workers=0 is safer on macOS/MPS and avoids extra memory\n",
    "hyb_batch = 32\n",
    "hyb_train_loader = DataLoader(hyb_train_ds, batch_size=hyb_batch, shuffle=True,  num_workers=0)\n",
    "hyb_dev_loader   = DataLoader(hyb_dev_ds,   batch_size=hyb_batch, shuffle=False, num_workers=0)\n",
    "hyb_test_loader  = DataLoader(hyb_test_ds,  batch_size=hyb_batch, shuffle=False, num_workers=0)\n",
    "\n",
    "xb_l, xb_c, yb = next(iter(hyb_train_loader))\n",
    "print(\"Hybrid LSTM batch:\", xb_l.shape, \" CNN batch:\", xb_c.shape, \" y:\", yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e075fff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridNet(\n",
      "  (cnn_branch): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (lstm_branch): LSTM(3, 64, batch_first=True)\n",
      "  (fc): Linear(in_features=96, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 16\n",
    "# HybridNet model: LSTM + CNN\n",
    "import torch.nn.functional as F\n",
    "class HybridNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        # LSTM branch\n",
    "        self.lstm_branch = nn.LSTM(\n",
    "            input_size=3, hidden_size=64, num_layers=1, batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(32 + 64, num_classes)\n",
    "\n",
    "    def forward(self, x_lstm, x_cnn):\n",
    "        out_l, _ = self.lstm_branch(x_lstm)\n",
    "        feat_l   = out_l[:, -1, :]\n",
    "        feat_c   = self.cnn_branch(x_cnn).view(x_cnn.size(0), -1)\n",
    "        return self.fc(torch.cat([feat_l, feat_c], dim=1))\n",
    "\n",
    "hyb_model = HybridNet(num_classes).to(device)\n",
    "print(hyb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9de1ba31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Hybrid for 20 epochs (early stopping enabled)\n"
     ]
    }
   ],
   "source": [
    "# 17\n",
    "# Hybrid training setup\n",
    "\n",
    "hyb_lr     = 1e-3\n",
    "hyb_epochs = 20  # lower while iterating if needed\n",
    "\n",
    "# Early stopping (on dev accuracy)\n",
    "hyb_es_patience  = 3        # stop after 3 epochs without improvement\n",
    "hyb_es_min_delta = 1e-4     # require at least this improvement\n",
    "hyb_es_wait      = 0\n",
    "\n",
    "hyb_criterion = nn.CrossEntropyLoss()\n",
    "hyb_optimizer = optim.Adam(hyb_model.parameters(), lr=hyb_lr)\n",
    "hyb_scheduler = nn.Sequential(\n",
    "    # use ReduceLROnPlateau via a tiny wrapper so we can keep the name variable\n",
    ")\n",
    "# Use the standard ReduceLROnPlateau directly:\n",
    "hyb_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    hyb_optimizer, mode=\"max\", factor=0.5, patience=2\n",
    ")\n",
    "\n",
    "best_hyb_dev_acc = -float(\"inf\")\n",
    "print(\"Training Hybrid for\", hyb_epochs, \"epochs (early stopping enabled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa0582ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/20  Train Loss: 0.5158  Val Loss: 0.3052  Val Acc: 0.8783  (best: 0.0000)\n",
      "  ðŸ”– New best Hybrid saved\n",
      "Epoch  2/20  Train Loss: 0.3323  Val Loss: 0.3177  Val Acc: 0.8748  (best: 0.8783)\n",
      "Epoch  3/20  Train Loss: 0.2677  Val Loss: 0.4006  Val Acc: 0.8257  (best: 0.8783)\n",
      "Epoch  4/20  Train Loss: 0.2309  Val Loss: 0.4210  Val Acc: 0.8318  (best: 0.8783)\n",
      "Early stopping after epoch 4 (no improvement for 3 epochs).\n"
     ]
    }
   ],
   "source": [
    "# 18\n",
    "# Hybrid Training Loop\n",
    "for epoch in range(1, hyb_epochs + 1):\n",
    "    hyb_model.train()\n",
    "    train_losses = []\n",
    "    for x_lstm, x_cnn, yb in hyb_train_loader:\n",
    "        x_lstm, x_cnn, yb = x_lstm.to(device), x_cnn.to(device), yb.to(device)\n",
    "        hyb_optimizer.zero_grad()\n",
    "        logits = hyb_model(x_lstm, x_cnn)\n",
    "        loss   = hyb_criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        hyb_optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    avg_train_loss = float(np.mean(train_losses)) if train_losses else 0.0\n",
    "\n",
    "    # Dev\n",
    "    hyb_model.eval()\n",
    "    val_preds, val_true, val_losses = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for x_lstm, x_cnn, yb in hyb_dev_loader:\n",
    "            x_lstm, x_cnn, yb = x_lstm.to(device), x_cnn.to(device), yb.to(device)\n",
    "            logits = hyb_model(x_lstm, x_cnn)\n",
    "            loss   = hyb_criterion(logits, yb)\n",
    "            val_losses.append(loss.item())\n",
    "            val_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "            val_true.extend(yb.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = float(np.mean(val_losses)) if val_losses else 0.0\n",
    "    val_acc      = accuracy_score(val_true, val_preds) if val_true else 0.0\n",
    "    hyb_scheduler.step(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}/{hyb_epochs}  Train Loss: {avg_train_loss:.4f}  \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}  Val Acc: {val_acc:.4f}  \"\n",
    "          f\"(best: {best_hyb_dev_acc if best_hyb_dev_acc!=-float('inf') else 0.0:.4f})\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_acc > best_hyb_dev_acc + hyb_es_min_delta:\n",
    "        best_hyb_dev_acc = val_acc\n",
    "        hyb_es_wait = 0\n",
    "        torch.save(hyb_model.state_dict(), PROC / \"best_hybrid.pth\")\n",
    "        print(\"  ðŸ”– New best Hybrid saved\")\n",
    "    else:\n",
    "        hyb_es_wait += 1\n",
    "        if hyb_es_wait >= hyb_es_patience:\n",
    "            print(f\"Early stopping after epoch {epoch} \"\n",
    "                  f\"(no improvement for {hyb_es_patience} epochs).\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4489e034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Test Accuracy: 0.6894\n",
      "\n",
      "Hybrid Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        bach       0.96      0.88      0.92      9581\n",
      "   beethoven       0.48      0.91      0.63      9581\n",
      "      chopin       0.89      0.59      0.71      9581\n",
      "      mozart       0.72      0.37      0.49      9581\n",
      "\n",
      "    accuracy                           0.69     38324\n",
      "   macro avg       0.76      0.69      0.69     38324\n",
      "weighted avg       0.76      0.69      0.69     38324\n",
      "\n",
      "\n",
      "Hybrid Confusion Matrix:\n",
      " [[8474  455  391  261]\n",
      " [   0 8708   26  847]\n",
      " [ 222 3411 5681  267]\n",
      " [  90 5680  255 3556]]\n"
     ]
    }
   ],
   "source": [
    "# 19\n",
    "# Hybrid Test Evaluation\n",
    "best_hyb = HybridNet(num_classes).to(device)\n",
    "best_hyb.load_state_dict(torch.load(PROC/\"best_hybrid.pth\", map_location=device))\n",
    "best_hyb.eval()\n",
    "\n",
    "hyb_preds, hyb_true = [], []\n",
    "with torch.no_grad():\n",
    "    for x_lstm, x_cnn, yb in hyb_test_loader:\n",
    "        x_lstm, x_cnn = x_lstm.to(device), x_cnn.to(device)\n",
    "        logits = best_hyb(x_lstm, x_cnn)\n",
    "        hyb_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "        hyb_true.extend(yb.numpy())\n",
    "\n",
    "print(f\"Hybrid Test Accuracy: {accuracy_score(hyb_true, hyb_preds):.4f}\\n\")\n",
    "print(\"Hybrid Classification Report:\\n\",\n",
    "      classification_report(hyb_true, hyb_preds, target_names=list(le.classes_), zero_division=0))\n",
    "print(\"\\nHybrid Confusion Matrix:\\n\", confusion_matrix(hyb_true, hyb_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
