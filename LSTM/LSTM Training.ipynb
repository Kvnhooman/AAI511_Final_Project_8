{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f28b1286",
   "metadata": {},
   "source": [
    "# Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fa678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Train: (480044, 50, 3), Dev: (41034, 50, 3), Test: (39295, 50, 3)\n",
      "Classes: ['bach' 'beethoven' 'chopin' 'mozart']\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "BASE_DIR   = Path('../data/processed_data')\n",
    "OUTPUT_DIR = BASE_DIR                   \n",
    "\n",
    "# Hyperparameters\n",
    "batch_size    = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs    = 20\n",
    "\n",
    "# Load data\n",
    "with open(BASE_DIR / 'lstm_train.pkl','rb') as f:\n",
    "    X_train, y_train = pickle.load(f)\n",
    "with open(BASE_DIR / 'lstm_dev.pkl','rb')   as f:\n",
    "    X_dev,   y_dev   = pickle.load(f)\n",
    "with open(BASE_DIR / 'lstm_test.pkl','rb')  as f:\n",
    "    X_test,  y_test  = pickle.load(f)\n",
    "\n",
    "with open(BASE_DIR / 'label_encoder.pkl','rb') as f:\n",
    "    le = pickle.load(f)\n",
    "num_classes = len(le.classes_)\n",
    "input_size  = X_train.shape[2]\n",
    "\n",
    "if   torch.backends.mps.is_available(): device = torch.device('mps')\n",
    "elif torch.cuda.is_available():         device = torch.device('cuda')\n",
    "else:                                   device = torch.device('cpu')\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Train: {X_train.shape}, Dev: {X_dev.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Classes: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f47cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One training batch X shape: torch.Size([32, 50, 3])\n",
      "One training batch y shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Convert to tensors\n",
    "X_tr = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_tr = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_dev_t = torch.tensor(X_dev, dtype=torch.float32).to(device)\n",
    "y_dev_t = torch.tensor(y_dev, dtype=torch.long).to(device)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_ds = TensorDataset(X_tr, y_tr)\n",
    "dev_ds   = TensorDataset(X_dev_t, y_dev_t)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "dev_loader   = DataLoader(dev_ds,   batch_size=batch_size, shuffle=False)\n",
    "#check check\n",
    "batch = next(iter(train_loader))\n",
    "print(\"One training batch X shape:\", batch[0].shape)\n",
    "print(\"One training batch y shape:\", batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c8ad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicLSTM(\n",
      "  (lstm): LSTM(3, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (fc): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MusicLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Take the output from the last time step\n",
    "        last_output = lstm_out[:, -1, :]     # shape: (batch, hidden_size)\n",
    "        return self.fc(last_output)          # shape: (batch, num_classes)\n",
    "\n",
    "# Hyperparameters for the model\n",
    "hidden_size = 128\n",
    "num_layers  = 2\n",
    "\n",
    "model = MusicLSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: CrossEntropyLoss()\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_dev_acc = 0.0\n",
    "\n",
    "print(\"Criterion:\", criterion)\n",
    "print(\"Optimizer:\", optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d840fd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/20  Train Loss: 0.1022  Dev Loss: 0.0374  Dev Acc: 0.9872\n",
      "  ðŸ”– New best model saved\n",
      "Epoch  2/20  Train Loss: 0.0450  Dev Loss: 0.0317  Dev Acc: 0.9882\n",
      "  ðŸ”– New best model saved\n",
      "Epoch  3/20  Train Loss: 0.0300  Dev Loss: 0.0421  Dev Acc: 0.9868\n",
      "Epoch  4/20  Train Loss: 0.0241  Dev Loss: 0.0311  Dev Acc: 0.9891\n",
      "  ðŸ”– New best model saved\n",
      "Epoch  5/20  Train Loss: 0.0212  Dev Loss: 0.0404  Dev Acc: 0.9849\n",
      "Epoch  6/20  Train Loss: 0.0187  Dev Loss: 0.0345  Dev Acc: 0.9891\n",
      "  ðŸ”– New best model saved\n",
      "Epoch  7/20  Train Loss: 0.0162  Dev Loss: 0.0520  Dev Acc: 0.9812\n",
      "Epoch  8/20  Train Loss: 0.0151  Dev Loss: 0.0594  Dev Acc: 0.9865\n",
      "Epoch  9/20  Train Loss: 0.0149  Dev Loss: 0.0520  Dev Acc: 0.9866\n",
      "Epoch 10/20  Train Loss: 0.0155  Dev Loss: 0.0273  Dev Acc: 0.9906\n",
      "  ðŸ”– New best model saved\n",
      "Epoch 11/20  Train Loss: 0.0148  Dev Loss: 0.0767  Dev Acc: 0.9807\n",
      "Epoch 12/20  Train Loss: 0.0151  Dev Loss: 0.0706  Dev Acc: 0.9830\n",
      "Epoch 13/20  Train Loss: 0.0138  Dev Loss: 0.0524  Dev Acc: 0.9877\n",
      "Epoch 14/20  Train Loss: 0.0141  Dev Loss: 0.0354  Dev Acc: 0.9889\n",
      "Epoch 15/20  Train Loss: 0.0134  Dev Loss: 0.0395  Dev Acc: 0.9876\n",
      "Epoch 16/20  Train Loss: 0.0144  Dev Loss: 0.0394  Dev Acc: 0.9883\n",
      "Epoch 17/20  Train Loss: 0.0130  Dev Loss: 0.0282  Dev Acc: 0.9913\n",
      "  ðŸ”– New best model saved\n",
      "Epoch 18/20  Train Loss: 0.0118  Dev Loss: 0.0392  Dev Acc: 0.9894\n",
      "Epoch 19/20  Train Loss: 0.0123  Dev Loss: 0.0380  Dev Acc: 0.9874\n",
      "Epoch 20/20  Train Loss: 0.0140  Dev Loss: 0.0511  Dev Acc: 0.9873\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    # â€”â€”â€” Trainingâ€”â€”â€”\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for Xb, yb in train_loader:\n",
    "        optimizer.zero_grad()            # reset gradients\n",
    "        logits = model(Xb)               # forward pass\n",
    "        loss   = criterion(logits, yb)   # compute loss\n",
    "        loss.backward()                  # backpropagate\n",
    "        optimizer.step()                 # update weights\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "    # â€”â€”â€” Validationâ€”â€”â€”\n",
    "    model.eval()\n",
    "    dev_preds, dev_true, dev_losses = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in dev_loader:\n",
    "            logits = model(Xb)\n",
    "            loss   = criterion(logits, yb)\n",
    "            dev_losses.append(loss.item())\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            dev_preds.extend(preds)\n",
    "            dev_true.extend(yb.cpu().numpy())\n",
    "\n",
    "    avg_dev_loss = np.mean(dev_losses)\n",
    "    dev_acc      = accuracy_score(dev_true, dev_preds)\n",
    "\n",
    "    #epoch results\n",
    "    print(f\"Epoch {epoch:2d}/{num_epochs}  \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}  \"\n",
    "          f\"Dev Loss: {avg_dev_loss:.4f}  \"\n",
    "          f\"Dev Acc: {dev_acc:.4f}\")\n",
    "\n",
    "    # Save best model by dev accuracy\n",
    "    if dev_acc > best_dev_acc:\n",
    "        best_dev_acc = dev_acc\n",
    "        torch.save(model.state_dict(), OUTPUT_DIR / 'best_model.pth')\n",
    "        print(\"  ðŸ”– New best model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db189fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.9734\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        bach       0.99      1.00      0.99     11313\n",
      "   beethoven       0.00      0.00      0.00         0\n",
      "      chopin       0.99      0.90      0.94      9581\n",
      "      mozart       0.96      1.00      0.98     18401\n",
      "\n",
      "    accuracy                           0.97     39295\n",
      "   macro avg       0.73      0.72      0.73     39295\n",
      "weighted avg       0.97      0.97      0.97     39295\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[11313     0     0     0]\n",
      " [    0     0     0     0]\n",
      " [  160     0  8602   819]\n",
      " [    0     0    68 18333]]\n"
     ]
    }
   ],
   "source": [
    "# Load the best model checkpoint\n",
    "best_model = MusicLSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "best_model.load_state_dict(torch.load(OUTPUT_DIR / 'best_model.pth'))\n",
    "best_model.eval()\n",
    "\n",
    "# Prepare test DataLoader\n",
    "X_te = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_te = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "test_loader = DataLoader(TensorDataset(X_te, y_te), batch_size=batch_size)\n",
    "\n",
    "# Collect predictions & true labels\n",
    "test_preds, test_true = [], []\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        logits = best_model(Xb)\n",
    "        preds  = logits.argmax(dim=1).cpu().numpy()\n",
    "        test_preds.extend(preds)\n",
    "        test_true.extend(yb.cpu().numpy())\n",
    "\n",
    "# accuracy\n",
    "test_acc = accuracy_score(test_true, test_preds)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\\n\")\n",
    "\n",
    "labels = list(range(num_classes))    # [0, 1, 2, 3]\n",
    "names  = le.classes_                 # ['bach', 'beethoven', 'chopin', 'mozart']\n",
    "\n",
    "print(\"Classification Report:\\n\",\n",
    "      classification_report(\n",
    "          test_true,\n",
    "          test_preds,\n",
    "          labels=labels,\n",
    "          target_names=names,\n",
    "          zero_division=0\n",
    "      ))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\",\n",
    "      confusion_matrix(\n",
    "          test_true,\n",
    "          test_preds,\n",
    "          labels=labels\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca0f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
