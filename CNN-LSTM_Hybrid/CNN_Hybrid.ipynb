{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80d1a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = Path('../data/processed_data')\n",
    "\n",
    "# Sequence length  (match your LSTM window size)\n",
    "WINDOW_LEN = 50\n",
    "\n",
    "# Pitch range for piano-roll\n",
    "PITCH_START = 21\n",
    "PITCH_END   = 108\n",
    "NUM_PITCHES = PITCH_END - PITCH_START + 1  # 88"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703da00",
   "metadata": {},
   "source": [
    "### Define Piano-Roll Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34151767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_pianoroll(seq,\n",
    "                     pitch_start=PITCH_START,\n",
    "                     num_pitches=NUM_PITCHES):\n",
    "    \"\"\"\n",
    "    seq: np.array of shape (WINDOW_LEN, 3), where seq[:,0] = pitch.\n",
    "    Returns a binary piano-roll: shape (1, num_pitches, WINDOW_LEN).\n",
    "    \"\"\"\n",
    "    pr = np.zeros((num_pitches, seq.shape[0]), dtype=np.float32)\n",
    "    for t, note in enumerate(seq):\n",
    "        p = int(note[0])\n",
    "        pr[p - pitch_start, t] = 1.0\n",
    "    return pr[np.newaxis, :, :] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd77559",
   "metadata": {},
   "source": [
    "### Load LSTM Windows & Build CNN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3ed5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: ['bach' 'beethoven' 'chopin' 'mozart']\n",
      "LSTM shapes: (480044, 50, 3) (41034, 50, 3) (39295, 50, 3)\n",
      "CNN shapes:  (480044, 1, 88, 50) (41034, 1, 88, 50) (39295, 1, 88, 50)\n",
      "Labels train/dev/test: (480044,) (41034,) (39295,)\n"
     ]
    }
   ],
   "source": [
    "# Load LSTM windows + labels\n",
    "with open(BASE_DIR/'lstm_train.pkl','rb') as f:\n",
    "    X_train_l, y_train = pickle.load(f)\n",
    "with open(BASE_DIR/'lstm_dev.pkl','rb') as f:\n",
    "    X_dev_l,   y_dev   = pickle.load(f)\n",
    "with open(BASE_DIR/'lstm_test.pkl','rb') as f:\n",
    "    X_test_l,  y_test  = pickle.load(f)\n",
    "with open(BASE_DIR/'label_encoder.pkl','rb') as f:\n",
    "    le = pickle.load(f)\n",
    "print(\"Label classes:\", le.classes_)\n",
    "# Convert each window to a piano-roll\n",
    "X_train_c = np.stack([seq_to_pianoroll(seq) for seq in X_train_l], axis=0)\n",
    "X_dev_c   = np.stack([seq_to_pianoroll(seq) for seq in X_dev_l],   axis=0)\n",
    "X_test_c  = np.stack([seq_to_pianoroll(seq) for seq in X_test_l],  axis=0)\n",
    "\n",
    "print(\"LSTM shapes:\", X_train_l.shape, X_dev_l.shape, X_test_l.shape)\n",
    "print(\"CNN shapes: \", X_train_c.shape, X_dev_c.shape, X_test_c.shape)\n",
    "print(\"Labels train/dev/test:\", y_train.shape, y_dev.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "827c8353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CNN feature files written.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.save(BASE_DIR/'cnn_train_data.npy',  X_train_c)\n",
    "np.save(BASE_DIR/'cnn_train_labels.npy', y_train)\n",
    "np.save(BASE_DIR/'cnn_dev_data.npy',    X_dev_c)\n",
    "np.save(BASE_DIR/'cnn_dev_labels.npy',   y_dev)\n",
    "np.save(BASE_DIR/'cnn_test_data.npy',   X_test_c)\n",
    "np.save(BASE_DIR/'cnn_test_labels.npy',  y_test)\n",
    "print(\"âœ… CNN feature files written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd977e73",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad22b059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM batch shape: torch.Size([32, 50, 3])\n",
      "CNN batch shape:  torch.Size([32, 1, 88, 50])\n",
      "Label batch shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Dataset & DataLoaders\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, lstm_data, cnn_data, labels):\n",
    "        self.lstm = torch.tensor(lstm_data, dtype=torch.float32)\n",
    "        self.cnn  = torch.tensor(cnn_data,  dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.lstm[idx], self.cnn[idx], self.labels[idx]\n",
    "\n",
    "# Build datasets\n",
    "train_ds = HybridDataset(X_train_l, X_train_c, y_train)\n",
    "dev_ds   = HybridDataset(X_dev_l,   X_dev_c,   y_dev)\n",
    "test_ds  = HybridDataset(X_test_l,  X_test_c,  y_test)\n",
    "\n",
    "# Create loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "dev_loader   = DataLoader(dev_ds,   batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "lstm_b, cnn_b, y_b = next(iter(train_loader))\n",
    "print(\"LSTM batch shape:\", lstm_b.shape)   \n",
    "print(\"CNN batch shape: \", cnn_b.shape)    \n",
    "print(\"Label batch shape:\", y_b.shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c562b",
   "metadata": {},
   "source": [
    "### Defining a Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "HybridNet(\n",
      "  (cnn_branch): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (lstm_branch): LSTM(3, 64, batch_first=True)\n",
      "  (fc): Linear(in_features=96, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Device selection \n",
    "device = torch.device('mps' if torch.backends.mps.is_available() \n",
    "                      else 'cuda' if torch.cuda.is_available() \n",
    "                      else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class HybridNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),                         \n",
    "            nn.Conv2d(16,32, kernel_size=3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))                \n",
    "        )\n",
    "        # LSTM branch\n",
    "        self.lstm_branch = nn.LSTM(\n",
    "            input_size=3,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # classifier\n",
    "        self.fc = nn.Linear(32 + 64, num_classes)\n",
    "\n",
    "    def forward(self, x_lstm, x_cnn):\n",
    "        # LSTM path\n",
    "        out_l, _ = self.lstm_branch(x_lstm)     \n",
    "        feat_l   = out_l[:, -1, :]             \n",
    "        # CNN path\n",
    "        feat_c   = self.cnn_branch(x_cnn)      \n",
    "        feat_c   = feat_c.view(feat_c.size(0), -1)  \n",
    "        # Concatenate and classify\n",
    "        combined = torch.cat([feat_l, feat_c], dim=1)  \n",
    "        return self.fc(combined)                       \n",
    "\n",
    "# Instantiate\n",
    "num_classes = len(le.classes_) \n",
    "model = HybridNet(num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b98bb",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c603f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epochs on device mps\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size    = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs    = 20\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion     = nn.CrossEntropyLoss()\n",
    "optimizer     = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Track best dev accuracy\n",
    "best_dev_acc  = 0.0\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs on device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2db50",
   "metadata": {},
   "source": [
    "### Hybrid Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c550c40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/20  Train Loss: 0.1003  Val Loss: 0.0239  Val Acc: 0.9922\n",
      "  ðŸ”– New best hybrid model saved\n",
      "Epoch  2/20  Train Loss: 0.0429  Val Loss: 0.0329  Val Acc: 0.9903\n",
      "Epoch  3/20  Train Loss: 0.0317  Val Loss: 0.0720  Val Acc: 0.9731\n",
      "Epoch  4/20  Train Loss: 0.0249  Val Loss: 0.0249  Val Acc: 0.9934\n",
      "  ðŸ”– New best hybrid model saved\n",
      "Epoch  5/20  Train Loss: 0.0203  Val Loss: 0.0265  Val Acc: 0.9911\n",
      "Epoch  6/20  Train Loss: 0.0172  Val Loss: 0.0238  Val Acc: 0.9905\n",
      "Epoch  7/20  Train Loss: 0.0155  Val Loss: 0.0266  Val Acc: 0.9916\n",
      "Epoch  8/20  Train Loss: 0.0144  Val Loss: 0.0365  Val Acc: 0.9910\n",
      "Epoch  9/20  Train Loss: 0.0132  Val Loss: 0.0313  Val Acc: 0.9919\n",
      "Epoch 10/20  Train Loss: 0.0118  Val Loss: 0.0342  Val Acc: 0.9916\n",
      "Epoch 11/20  Train Loss: 0.0109  Val Loss: 0.0328  Val Acc: 0.9927\n",
      "Epoch 12/20  Train Loss: 0.0104  Val Loss: 0.0240  Val Acc: 0.9929\n",
      "Epoch 13/20  Train Loss: 0.0099  Val Loss: 0.0377  Val Acc: 0.9910\n",
      "Epoch 14/20  Train Loss: 0.0089  Val Loss: 0.0335  Val Acc: 0.9916\n",
      "Epoch 15/20  Train Loss: 0.0082  Val Loss: 0.0309  Val Acc: 0.9931\n",
      "Epoch 16/20  Train Loss: 0.0085  Val Loss: 0.0337  Val Acc: 0.9919\n",
      "Epoch 17/20  Train Loss: 0.0081  Val Loss: 0.0282  Val Acc: 0.9930\n",
      "Epoch 18/20  Train Loss: 0.0083  Val Loss: 0.0369  Val Acc: 0.9878\n",
      "Epoch 19/20  Train Loss: 0.0093  Val Loss: 0.0295  Val Acc: 0.9930\n",
      "Epoch 20/20  Train Loss: 0.0083  Val Loss: 0.0252  Val Acc: 0.9913\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    # â€”â€”â€” Training â€”â€”â€”\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for x_lstm, x_cnn, yb in train_loader:\n",
    "        x_lstm, x_cnn, yb = x_lstm.to(device), x_cnn.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_lstm, x_cnn)\n",
    "        loss   = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "    # â€”â€”â€” Validation â€”â€”â€”\n",
    "    model.eval()\n",
    "    val_preds, val_true, val_losses = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for x_lstm, x_cnn, yb in dev_loader:\n",
    "            x_lstm, x_cnn, yb = x_lstm.to(device), x_cnn.to(device), yb.to(device)\n",
    "            logits = model(x_lstm, x_cnn)\n",
    "            loss   = criterion(logits, yb)\n",
    "            val_losses.append(loss.item())\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_true.extend(yb.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    val_acc      = accuracy_score(val_true, val_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}/{num_epochs}  \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}  \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}  \"\n",
    "          f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_dev_acc:\n",
    "        best_dev_acc = val_acc\n",
    "        torch.save(model.state_dict(), BASE_DIR / 'best_hybrid.pth')\n",
    "        print(\"  ðŸ”– New best hybrid model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948767be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
