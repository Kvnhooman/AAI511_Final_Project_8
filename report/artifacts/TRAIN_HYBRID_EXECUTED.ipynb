{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc39baf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T10:39:24.615536Z",
     "iopub.status.busy": "2025-08-11T10:39:24.614654Z",
     "iopub.status.idle": "2025-08-11T10:39:24.621258Z",
     "shell.execute_reply": "2025-08-11T10:39:24.620427Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"/content/drive/MyDrive/ComposerReport/report/processed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2361be59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T10:39:24.625978Z",
     "iopub.status.busy": "2025-08-11T10:39:24.625353Z",
     "iopub.status.idle": "2025-08-11T10:39:24.715625Z",
     "shell.execute_reply": "2025-08-11T10:39:24.713374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYBRID BASE_DIR -> /content/drive/MyDrive/ComposerReport/report/processed_data\n",
      "Files in BASE_DIR: []\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "lstm_data.pkl not found in /content/drive/MyDrive/ComposerReport/report/processed_data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3519094063.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Files in BASE_DIR:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"lstm_data.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lstm_data.pkl not found in \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"lstm_dev.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m\"lstm_dev.pkl not found in \"\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"lstm_test.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lstm_test.pkl not found in \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: lstm_data.pkl not found in /content/drive/MyDrive/ComposerReport/report/processed_data"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path, PurePath\n",
    "BASE_DIR = Path('/content/drive/MyDrive/ComposerReport/report/processed_data')\n",
    "print(\"HYBRID BASE_DIR ->\", BASE_DIR)\n",
    "import os\n",
    "print(\"Files in BASE_DIR:\", sorted([f for f in os.listdir(BASE_DIR) if f.endswith(\".pkl\")]))\n",
    "assert (BASE_DIR/\"lstm_data.pkl\").exists(), \"lstm_data.pkl not found in \" + str(BASE_DIR)\n",
    "assert (BASE_DIR/\"lstm_dev.pkl\").exists(),  \"lstm_dev.pkl not found in \"  + str(BASE_DIR)\n",
    "assert (BASE_DIR/\"lstm_test.pkl\").exists(), \"lstm_test.pkl not found in \" + str(BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6b643c",
   "metadata": {
    "id": "2c6b643c",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "BASE_DIR = Path(BASE_DIR)  # ensures you can do BASE_DIR / 'file.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d1c77",
   "metadata": {
    "id": "a58d1c77"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'hybrid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0cc6c",
   "metadata": {
    "id": "cec0cc6c"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import zipfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tZf2sm59bqg8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "error",
     "timestamp": 1754905769067,
     "user": {
      "displayName": "Devin Eror",
      "userId": "08010257872820414663"
     },
     "user_tz": 420
    },
    "id": "tZf2sm59bqg8",
    "outputId": "df789a35-1a49-4868-935c-97a6832d8172"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/cnn_data.pkl.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-268616791.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mextract_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/processed_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/cnn_data.pkl.zip'"
     ]
    }
   ],
   "source": [
    "zip_path = '/content/cnn_data.pkl.zip'\n",
    "extract_to = '/content/processed_data'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "  zip_ref.extractall(extract_to)\n",
    "\n",
    "print(\"Unzipped to:\", extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1a67b",
   "metadata": {
    "id": "80d1a67b"
   },
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = Path('/content')\n",
    "\n",
    "# Sequence length  (match your LSTM window size)\n",
    "WINDOW_LEN = 50\n",
    "\n",
    "# Pitch range for piano-roll\n",
    "PITCH_START = 21\n",
    "PITCH_END   = 108\n",
    "NUM_PITCHES = PITCH_END - PITCH_START + 1  # 88"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703da00",
   "metadata": {
    "id": "4703da00"
   },
   "source": [
    "### Define Piano-Roll Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34151767",
   "metadata": {
    "id": "34151767"
   },
   "outputs": [],
   "source": [
    "def seq_to_pianoroll(seq,\n",
    "                     pitch_start=PITCH_START,\n",
    "                     num_pitches=NUM_PITCHES):\n",
    "    \"\"\"\n",
    "    seq: np.array of shape (WINDOW_LEN, 3), where seq[:,0] = pitch.\n",
    "    Returns a binary piano-roll: shape (1, num_pitches, WINDOW_LEN).\n",
    "    \"\"\"\n",
    "    pr = np.zeros((num_pitches, seq.shape[0]), dtype=np.float32)\n",
    "    for t, note in enumerate(seq):\n",
    "        p = int(note[0])\n",
    "        pr[p - pitch_start, t] = 1.0\n",
    "    return pr[np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd77559",
   "metadata": {
    "id": "edd77559"
   },
   "source": [
    "### Load LSTM Windows & Build CNN Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os, pickle, numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ensure BASE_DIR is a Path\n",
    "if not isinstance(BASE_DIR, Path):\n",
    "    BASE_DIR = Path(BASE_DIR)\n",
    "\n",
    "with open(BASE_DIR / 'lstm_data.pkl','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "def first_non_none(*vals):\n",
    "    for v in vals:\n",
    "        if v is not None:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "# Support tuple/list or dict-shaped pickles\n",
    "if isinstance(data, dict):\n",
    "    X_lstm = first_non_none(data.get('X'), data.get('X_lstm'), data.get('X_windows'))\n",
    "    y      = first_non_none(data.get('y'), data.get('labels'))\n",
    "    le     = first_non_none(data.get('le'), data.get('label_encoder'))\n",
    "else:\n",
    "    if len(data) == 3:\n",
    "        X_lstm, y, le = data\n",
    "    elif len(data) == 2:\n",
    "        X_lstm, y = data\n",
    "        le = None\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected lstm_data.pkl format: len={len(data)}\")\n",
    "\n",
    "print(\"Type(X_lstm):\", type(X_lstm))\n",
    "try: print(\"len(X_lstm):\", len(X_lstm))\n",
    "except TypeError: print(\"X_lstm shape:\", getattr(X_lstm, 'shape', 'N/A'))\n",
    "print(\"Type(y):\", type(y), \"len(y):\", len(y) if hasattr(y, '__len__') else 'N/A')\n",
    "if getattr(le, \"classes_\", None) is not None:\n",
    "    print(\"Label classes:\", le.classes_)\n",
    "\n",
    "assert hasattr(X_lstm, '__len__') and len(X_lstm) == len(y), \"X and y must be same length\"\n",
    "assert len(X_lstm) > 0, \"X_lstm is empty â€” upstream preprocessing likely failed.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ed5a3",
   "metadata": {
    "id": "cc3ed5a3"
   },
   "outputs": [],
   "source": [
    "# --- Unified Data Loading & Conversion (tuple/dict-safe) ---\n",
    "from pathlib import Path\n",
    "import pickle, numpy as np\n",
    "\n",
    "def _pick_first(d, keys):\n",
    "    for k in keys:\n",
    "        if k in d and d[k] is not None:\n",
    "            return d[k]\n",
    "    return None\n",
    "\n",
    "def load_splits(base: Path):\n",
    "    # Optional label encoder\n",
    "    le = None\n",
    "    le_path = base / 'label_encoder.pkl'\n",
    "    if le_path.exists():\n",
    "        with open(le_path, 'rb') as f:\n",
    "            le = pickle.load(f)\n",
    "\n",
    "    # Train split can be tuple OR dict depending on your preprocessing run\n",
    "    with open(base / 'lstm_data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        X_train_l = _pick_first(data, ['X','X_lstm','X_windows'])\n",
    "        y_train   = _pick_first(data, ['y','labels'])\n",
    "        le_in     = _pick_first(data, ['le','label_encoder'])\n",
    "        if le is None: le = le_in\n",
    "    else:\n",
    "        # Expect (X_train_l, y_train, le) or (X_train_l, y_train)\n",
    "        if len(data) == 3:\n",
    "            X_train_l, y_train, le_in = data\n",
    "            if le is None: le = le_in\n",
    "        elif len(data) == 2:\n",
    "            X_train_l, y_train = data\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected lstm_data.pkl format: len={len(data)}\")\n",
    "\n",
    "    # Dev/Test are expected as tuples\n",
    "    with open(base / 'lstm_dev.pkl', 'rb') as f:\n",
    "        X_dev_l, y_dev = pickle.load(f)\n",
    "    with open(base / 'lstm_test.pkl', 'rb') as f:\n",
    "        X_test_l, y_test = pickle.load(f)\n",
    "\n",
    "    if hasattr(le, \"classes_\"):\n",
    "        print(\"Label classes:\", le.classes_)\n",
    "    else:\n",
    "        print(\"[warn] Label encoder missing or has no classes_\")\n",
    "\n",
    "    return X_train_l, y_train, X_dev_l, y_dev, X_test_l, y_test, le\n",
    "\n",
    "def to_pianoroll_batch(seqs):\n",
    "    # assumes seq_to_pianoroll(seq) is defined earlier in the notebook\n",
    "    return np.stack([seq_to_pianoroll(s) for s in seqs], axis=0)\n",
    "\n",
    "# BASE_DIR must be set (or injected by the orchestrator) to your processed_data folder\n",
    "X_train_l, y_train, X_dev_l, y_dev, X_test_l, y_test, le = load_splits(BASE_DIR)\n",
    "\n",
    "X_train_c = to_pianoroll_batch(X_train_l)\n",
    "X_dev_c   = to_pianoroll_batch(X_dev_l)\n",
    "X_test_c  = to_pianoroll_batch(X_test_l)\n",
    "\n",
    "print(\"LSTM shapes:\", getattr(X_train_l, 'shape', len(X_train_l)), getattr(X_dev_l, 'shape', len(X_dev_l)), getattr(X_test_l, 'shape', len(X_test_l)))\n",
    "print(\"CNN shapes: \", X_train_c.shape, X_dev_c.shape, X_test_c.shape)\n",
    "print(\"Labels train/dev/test:\", len(y_train), len(y_dev), len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c8353",
   "metadata": {
    "id": "827c8353"
   },
   "outputs": [],
   "source": [
    "\n",
    "np.save(BASE_DIR/'cnn_train_data.npy',  X_train_c)\n",
    "np.save(BASE_DIR/'cnn_train_labels.npy', y_train)\n",
    "np.save(BASE_DIR/'cnn_dev_data.npy',    X_dev_c)\n",
    "np.save(BASE_DIR/'cnn_dev_labels.npy',   y_dev)\n",
    "np.save(BASE_DIR/'cnn_test_data.npy',   X_test_c)\n",
    "np.save(BASE_DIR/'cnn_test_labels.npy',  y_test)\n",
    "print(\"âœ… CNN feature files written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd977e73",
   "metadata": {
    "id": "fd977e73"
   },
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22b059",
   "metadata": {
    "id": "ad22b059"
   },
   "outputs": [],
   "source": [
    "# Hybrid Dataset & DataLoaders\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, lstm_data, cnn_data, labels):\n",
    "        self.lstm = torch.tensor(lstm_data, dtype=torch.float32)\n",
    "        self.cnn  = torch.tensor(cnn_data,  dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.lstm[idx], self.cnn[idx], self.labels[idx]\n",
    "\n",
    "# Build datasets\n",
    "train_ds = HybridDataset(X_train_l, X_train_c, y_train)\n",
    "dev_ds   = HybridDataset(X_dev_l,   X_dev_c,   y_dev)\n",
    "test_ds  = HybridDataset(X_test_l,  X_test_c,  y_test)\n",
    "\n",
    "# Create loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "dev_loader   = DataLoader(dev_ds,   batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n",
    "\n",
    "\n",
    "lstm_b, cnn_b, y_b = next(iter(train_loader))\n",
    "print(\"LSTM batch shape:\", lstm_b.shape)\n",
    "print(\"CNN batch shape: \", cnn_b.shape)\n",
    "print(\"Label batch shape:\", y_b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c562b",
   "metadata": {
    "id": "1f2c562b"
   },
   "source": [
    "### Defining a Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0f604",
   "metadata": {
    "id": "68a0f604"
   },
   "outputs": [],
   "source": [
    "# Device selection\n",
    "device = torch.device('mps' if torch.backends.mps.is_available()\n",
    "                      else 'cuda' if torch.cuda.is_available()\n",
    "                      else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class HybridNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(16,32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        # LSTM branch\n",
    "        self.lstm_branch = nn.LSTM(\n",
    "            input_size=3,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # classifier\n",
    "        self.fc = nn.Linear(32 + 64, num_classes)\n",
    "\n",
    "    def forward(self, x_lstm, x_cnn):\n",
    "        # LSTM path\n",
    "        out_l, _ = self.lstm_branch(x_lstm)\n",
    "        feat_l   = out_l[:, -1, :]\n",
    "        # CNN path\n",
    "        feat_c   = self.cnn_branch(x_cnn)\n",
    "        feat_c   = feat_c.view(feat_c.size(0), -1)\n",
    "        # Concatenate and classify\n",
    "        combined = torch.cat([feat_l, feat_c], dim=1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "# Instantiate\n",
    "num_classes = len(le.classes_)\n",
    "model = HybridNet(num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b98bb",
   "metadata": {
    "id": "cb1b98bb"
   },
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c603f5a",
   "metadata": {
    "id": "8c603f5a"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size    = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs    = 20\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion     = nn.CrossEntropyLoss()\n",
    "optimizer     = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Track best dev accuracy\n",
    "best_dev_acc  = 0.0\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs on device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2db50",
   "metadata": {
    "id": "d3b2db50"
   },
   "source": [
    "### Hybrid Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550c40e",
   "metadata": {
    "id": "c550c40e"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    # â€”â€”â€” Training â€”â€”â€”\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for x_lstm, x_cnn, yb in train_loader:\n",
    "        x_lstm, x_cnn, yb = x_lstm.to(device), x_cnn.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_lstm, x_cnn)\n",
    "        loss   = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "    # â€”â€”â€” Validation â€”â€”â€”\n",
    "    model.eval()\n",
    "    val_preds, val_true, val_losses = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for x_lstm, x_cnn, yb in dev_loader:\n",
    "            x_lstm, x_cnn, yb = x_lstm.to(device), x_cnn.to(device), yb.to(device)\n",
    "            logits = model(x_lstm, x_cnn)\n",
    "            loss   = criterion(logits, yb)\n",
    "            val_losses.append(loss.item())\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_true.extend(yb.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    val_acc      = accuracy_score(val_true, val_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch:2d}/{num_epochs}  \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}  \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}  \"\n",
    "          f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_dev_acc:\n",
    "        best_dev_acc = val_acc\n",
    "        torch.save(model.state_dict(), BASE_DIR / 'best_hybrid.pth')\n",
    "        print(\"  ðŸ”– New best hybrid model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948767be",
   "metadata": {
    "id": "948767be"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c0470",
   "metadata": {
    "id": "d10c0470"
   },
   "outputs": [],
   "source": [
    "\n",
    "import json, os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def _safe_metrics_dump(model_name, y_true_names, y_pred_names, labels):\n",
    "    acc = accuracy_score(y_true_names, y_pred_names)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_true_names, y_pred_names, average='macro', zero_division=0)\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(y_true_names, y_pred_names, average='weighted', zero_division=0)\n",
    "    report = classification_report(y_true_names, y_pred_names, digits=4, output_dict=True)\n",
    "    outdir = Path('/mnt/data/artifacts')\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    with open(outdir / f'metrics_{model_name}.json', 'w') as f:\n",
    "        json.dump({\n",
    "            \"model\": model_name,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision_macro\": precision_macro,\n",
    "            \"recall_macro\": recall_macro,\n",
    "            \"f1_macro\": f1_macro,\n",
    "            \"precision_weighted\": precision_weighted,\n",
    "            \"recall_weighted\": recall_weighted,\n",
    "            \"f1_weighted\": f1_weighted,\n",
    "            \"classification_report\": report,\n",
    "            \"labels\": list(labels)\n",
    "        }, f, indent=2)\n",
    "    cm = confusion_matrix(y_true_names, y_pred_names, labels=labels)\n",
    "    plt.figure()\n",
    "    im = plt.imshow(cm)\n",
    "    plt.colorbar(im)\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    for (i, j), val in np.ndenumerate(cm):\n",
    "        plt.text(j, i, int(val), ha='center', va='center')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / f'confusion_{model_name}.png', dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "try:\n",
    "    _labels = le.classes_ if 'le' in globals() else sorted(set(y_true_names) | set(y_pred_names))\n",
    "    _model_name = MODEL_NAME if 'MODEL_NAME' in globals() else 'model'\n",
    "    _safe_metrics_dump(_model_name, y_true_names, y_pred_names, _labels)\n",
    "    print(f\"[patch] Saved metrics and confusion matrix for {_model_name} to /mnt/data/artifacts\")\n",
    "except Exception as e:\n",
    "    print(\"[patch] Unable to compute metrics from this notebook:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
