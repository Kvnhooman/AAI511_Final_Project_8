{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b94395f",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "BASE_DIR = Path(\"./processed_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5938c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'eval'  # used for artifact filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sxnvrKmcOQC"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "eRjVTTEcWk6w",
    "outputId": "a4c7069a-03a0-40c7-f314-683b86053f07"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1494382530.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHybridModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHybridModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_hybrid.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from model import HybridModel\n",
    "model = HybridModel()\n",
    "model.load_state_dict(torch.load('best_hybrid.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tW0Ny2tsWl3d"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "# If needed: reload test set (X_test_l, X_test_c, y_test), rewrap into test_loader\n",
    "\n",
    "# Run inference\n",
    "test_preds, test_true = [], []\n",
    "with torch.no_grad():\n",
    "    for x_lstm, x_cnn, yb in test_loader:\n",
    "        x_lstm, x_cnn = x_lstm.to(device), x_cnn.to(device)\n",
    "        logits = model(x_lstm, x_cnn)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        test_preds.extend(preds)\n",
    "        test_true.extend(yb.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWq9bt1d0UrU"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "y_true_names = le.inverse_transform(test_true)\n",
    "y_pred_names = le.inverse_transform(test_preds)\n",
    "\n",
    "print(classification_report(y_true_names, y_pred_names, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true_names, y_pred_names, labels=le.classes_)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c0470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def _safe_metrics_dump(model_name, y_true_names, y_pred_names, labels):\n",
    "    acc = accuracy_score(y_true_names, y_pred_names)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_true_names, y_pred_names, average='macro', zero_division=0)\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(y_true_names, y_pred_names, average='weighted', zero_division=0)\n",
    "    report = classification_report(y_true_names, y_pred_names, digits=4, output_dict=True)\n",
    "    outdir = Path('/mnt/data/artifacts')\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    with open(outdir / f'metrics_{model_name}.json', 'w') as f:\n",
    "        json.dump({\n",
    "            \"model\": model_name,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision_macro\": precision_macro,\n",
    "            \"recall_macro\": recall_macro,\n",
    "            \"f1_macro\": f1_macro,\n",
    "            \"precision_weighted\": precision_weighted,\n",
    "            \"recall_weighted\": recall_weighted,\n",
    "            \"f1_weighted\": f1_weighted,\n",
    "            \"classification_report\": report,\n",
    "            \"labels\": list(labels)\n",
    "        }, f, indent=2)\n",
    "    cm = confusion_matrix(y_true_names, y_pred_names, labels=labels)\n",
    "    plt.figure()\n",
    "    im = plt.imshow(cm)\n",
    "    plt.colorbar(im)\n",
    "    plt.xticks(range(len(labels)), labels, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "    for (i, j), val in np.ndenumerate(cm):\n",
    "        plt.text(j, i, int(val), ha='center', va='center')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / f'confusion_{model_name}.png', dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "try:\n",
    "    _labels = le.classes_ if 'le' in globals() else sorted(set(y_true_names) | set(y_pred_names))\n",
    "    _model_name = MODEL_NAME if 'MODEL_NAME' in globals() else 'model'\n",
    "    _safe_metrics_dump(_model_name, y_true_names, y_pred_names, _labels)\n",
    "    print(f\"[patch] Saved metrics and confusion matrix for {_model_name} to /mnt/data/artifacts\")\n",
    "except Exception as e:\n",
    "    print(\"[patch] Unable to compute metrics from this notebook:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
